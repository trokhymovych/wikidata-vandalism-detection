{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148ddcf-0412-4d40-97ee-4fed7f609c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata\n",
    "\n",
    "spark = wmfdata.spark.create_custom_session(\n",
    "    master=\"yarn\",\n",
    "    ship_python_env = True,\n",
    "    app_name=f'mediawiki history',\n",
    "    spark_config={\n",
    "        \"spark.driver.memory\": \"16g\",\n",
    "        \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.memoryOverhead\": \"12g\",\n",
    "        \"spark.executor.cores\":2,\n",
    "        \"spark.sql.shuffle.partitions\": 256, #2048\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 20971520,\n",
    "        \"spark.driver.maxResultSize\": \"2g\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93dee1-951c-4c26-802b-2ba8a5f6926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the recent partitions\n",
    "spark.sql('SHOW PARTITIONS wmf.mediawiki_wikitext_history').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96eee41-8b7d-4498-8e60-95b94e958faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, broadcast\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import rand\n",
    "import re \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "from collections.abc import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@udf(\"String\")\n",
    "def findProp(t):\n",
    "    try:\n",
    "        return re.search(\"Property:(\\w+)\\]\",t).group(1)\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "@udf(\"String\")\n",
    "def findCommentSummary(t):\n",
    "    try:\n",
    "        return re.search(\"\\* ([\\w-]+)\",t).group(1)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "@udf(\"String\")\n",
    "def getObject(t):\n",
    "    try:\n",
    "        return re.search(r\"(Q\\w+)\",t).group(1)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "@udf(\"String\")\n",
    "def getValue(t):\n",
    "    try:\n",
    "        return eval(t).get('id',None)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02052404-9c82-4cda-906a-012a7a9380fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the recent partitions\n",
    "spark.sql('SHOW PARTITIONS wmf.mediawiki_history').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f118c7-209f-4e09-a86e-947697cd9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the recent partitions\n",
    "spark.sql('SHOW PARTITIONS wmf.wikidata_item_page_link').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78270785-8d62-4acc-a265-3a575ce92ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the recent partitions\n",
    "res = spark.sql('SHOW PARTITIONS wmf.mediawiki_wikitext_history').toPandas()\n",
    "[i for i in res.partition.values if \"wikidata\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddc9a94-c789-499f-bac3-2e1c4bd394ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_small = \"2024-05-06\"\n",
    "snapshot_big = \"2024-04\"\n",
    "lang='en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db064b-caff-465d-8c7c-1f4fc320a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 12:24:35 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/06/10 12:24:56 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reverted revisions:  1114789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 12:28:54 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bot', 'bureaucrat', 'checkuser', 'confirmed', 'flood', 'interface-admin', 'ipblock-exempt', 'propertycreator', 'rollbacker', 'suppress', 'sysop', 'translationadmin', 'wikidata-staff']\n",
      "['group', 'name']\n",
      "['revision_tags-InfoboxExport gadget', 'revision_tags-Unexpected value for IMDb identifier', 'revision_tags-Unexpected value for IUCN conservation status', 'revision_tags-Unexpected value for gender', 'revision_tags-deprecated property', 'revision_tags-description-phone-number', 'revision_tags-emoji', 'revision_tags-mw-replace', 'revision_tags-new editor changing statement', 'revision_tags-new editor removing statement', 'revision_tags-possible test edit', 'revision_tags-possible vandalism', 'revision_tags-removal of gender property', 'revision_tags-removing deprecated statement', 'revision_tags-self-referencing', 'revision_tags-unexpected value for phone number', 'revision_tags-unsourced ethnicity addition by IP', 'revision_tags-wikisyntax']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[page_title: string, event_timestamp: timestamp, revision_id: bigint, event_comment: string, revision_parent_id: bigint, is_bot: int, revision_is_identity_reverted: boolean, revision_is_identity_revert: boolean, revision_seconds_to_identity_revert: bigint, revision_first_identity_reverting_revision_id: bigint, event_user_text: string, number_of_groups: int, event_user_revision_count: bigint, event_user_is_anonymous: boolean, page_revision_count: bigint, page_seconds_since_previous_revision: bigint, user_age: int, page_age: int, reverting_revision_is_reverted_revision: boolean, reverting_revision_event_user_text: string, self_revert: boolean, id: string, label_en: string, instanceof: array<string>, rank: array<string>, event_user_groups-bot: int, event_user_groups-bureaucrat: int, event_user_groups-checkuser: int, event_user_groups-confirmed: int, event_user_groups-flood: int, event_user_groups-interface-admin: int, event_user_groups-ipblock-exempt: int, event_user_groups-propertycreator: int, event_user_groups-rollbacker: int, event_user_groups-suppress: int, event_user_groups-sysop: int, event_user_groups-translationadmin: int, event_user_groups-wikidata-staff: int, event_user_is_bot_by-group: int, event_user_is_bot_by-name: int, revision_tags-revision_tags-InfoboxExport-gadget: int, revision_tags-revision_tags-Unexpected-value-for-IMDb-identifier: int, revision_tags-revision_tags-Unexpected-value-for-IUCN-conservation-status: int, revision_tags-revision_tags-Unexpected-value-for-gender: int, revision_tags-revision_tags-deprecated-property: int, revision_tags-revision_tags-description-phone-number: int, revision_tags-revision_tags-emoji: int, revision_tags-revision_tags-mw-replace: int, revision_tags-revision_tags-new-editor-changing-statement: int, revision_tags-revision_tags-new-editor-removing-statement: int, revision_tags-revision_tags-possible-test-edit: int, revision_tags-revision_tags-possible-vandalism: int, revision_tags-revision_tags-removal-of-gender-property: int, revision_tags-revision_tags-removing-deprecated-statement: int, revision_tags-revision_tags-self-referencing: int, revision_tags-revision_tags-unexpected-value-for-phone-number: int, revision_tags-revision_tags-unsourced-ethnicity-addition-by-IP: int, revision_tags-revision_tags-wikisyntax: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsWithSiteLink =  spark.sql (f'''\n",
    "SELECT DISTINCT item_id FROM  wmf.wikidata_item_page_link WHERE \n",
    "snapshot = '{snapshot_small}'\n",
    "''')\n",
    "\n",
    "\n",
    "claims =  spark.sql(f'''SELECT TIMESTAMP(event_timestamp), \n",
    "                                    revision_id,\n",
    "                                    page_title,\n",
    "                                    event_comment,\n",
    "                                    revision_parent_id, \n",
    "                                    event_user_is_bot_by,\n",
    "                                    SIZE(event_user_is_bot_by) as is_bot,\n",
    "                                    revision_is_identity_reverted,\n",
    "                                    revision_is_identity_revert,\n",
    "                                    revision_seconds_to_identity_revert,\n",
    "                                    revision_first_identity_reverting_revision_id,\n",
    "                                    event_user_groups,\n",
    "                                    event_user_text,\n",
    "                                    SIZE(event_user_groups) as number_of_groups,\n",
    "                                    event_user_revision_count,\n",
    "                                    event_user_is_anonymous,                                    \n",
    "                                    page_revision_count,\n",
    "                                    page_seconds_since_previous_revision,\n",
    "                                    datediff(TIMESTAMP(event_timestamp),TIMESTAMP(event_user_creation_timestamp)) AS user_age,\n",
    "                                    datediff(TIMESTAMP(event_timestamp),TIMESTAMP(page_first_edit_timestamp)) AS page_age,\n",
    "                                    revision_tags\n",
    "                                    \n",
    "            FROM wmf.mediawiki_history WHERE event_entity = 'revision' \n",
    "            AND wiki_db = 'wikidatawiki' \n",
    "            AND page_namespace=0\n",
    "            AND snapshot = '{snapshot_big}' \n",
    "            AND TIMESTAMP(event_timestamp) BETWEEN TIMESTAMP('2021-01-01') AND TIMESTAMP('2024-01-01')\n",
    "            ''')\n",
    "    \n",
    "# collecting info about reverting revision\n",
    "columns_to_select = claims.columns\n",
    "changes_new = claims.alias(\"t1\") \\\n",
    "    .join(claims.alias(\"t2\"), F.col(\"t1.revision_first_identity_reverting_revision_id\") == F.col(\"t2.revision_id\"), \"left\") \\\n",
    "    .select(\n",
    "        *[f\"t1.{c}\" for c in claims.columns],\n",
    "        F.col(\"t2.revision_is_identity_reverted\").alias(\"reverting_revision_is_reverted_revision\"),\n",
    "        F.col(\"t2.event_user_text\").alias(\"reverting_revision_event_user_text\"),\n",
    "    )\n",
    "changes_new = changes_new.withColumn(\n",
    "    \"self_revert\", \n",
    "    (F.col(\"event_user_text\") == F.col(\"reverting_revision_event_user_text\")\n",
    "    )\n",
    ")\n",
    "changes_new = changes_new.filter(\n",
    "    F.array_contains(\"revision_tags\", \"wikidata-ui\") \n",
    ")\n",
    "\n",
    "\n",
    "##### Sampling strategy: \n",
    "# 1. Leave all the reverts\n",
    "changes_new_reverts = changes_new.where(F.col(\"revision_is_identity_reverted\") == True)\n",
    "n_reverts = changes_new_reverts.count()\n",
    "print(\"Number of reverted revisions: \", n_reverts)\n",
    "\n",
    "# 2. Leave all the ids from testset\n",
    "holdout_test = pd.read_csv(\"../data/holdout/test_holdout.csv\", sep=\"\\t\")\n",
    "revisions_to_check = list(set(holdout_test.rev))\n",
    "changes_new_holdout = changes_new.filter(F.col(\"revision_id\").isin(revisions_to_check))\n",
    "\n",
    "# 3. Sample not-reverted revisions (10 times number of reverts)\n",
    "changes_new_non_reverts = changes_new.where(F.col(\"revision_is_identity_reverted\") == False)\n",
    "changes_new_non_reverts = changes_new_non_reverts.sample(fraction=1.0, seed=42).limit(n_reverts * 5)\n",
    "\n",
    "# 4. Union dataframes: \n",
    "changes_new = changes_new_reverts.union(changes_new_holdout).union(changes_new_non_reverts)\n",
    "\n",
    "# 5. Drop duplicates: \n",
    "changes_new = changes_new.dropDuplicates()\n",
    "####\n",
    "\n",
    "\n",
    "#### Additional features: \n",
    "instanceof = spark.sql(f\"\"\"\n",
    "SELECT subject as page_title, rank, object FROM (\n",
    "    SELECT\n",
    "      subject,\n",
    "      claim.rank,\n",
    "      claim.mainSnak.property AS predicate,\n",
    "      claim.mainSnak.dataValue.value AS object\n",
    "    FROM (\n",
    "          SELECT\n",
    "            id as subject,\n",
    "            explode(claims) as claim\n",
    "          FROM wmf.wikidata_entity\n",
    "          WHERE snapshot = '{snapshot_small}'\n",
    "    ) t\n",
    " ) \n",
    " WHERE predicate = \"P31\"\n",
    "\"\"\")\n",
    "\n",
    "labels = spark.sql(f\"\"\"SELECT id, labels.{lang} as label_en\n",
    "        FROM wmf.wikidata_entity\n",
    "      WHERE snapshot = '{snapshot_small}'\n",
    "      \"\"\")\n",
    "\n",
    "changes_new = changes_new.join(labels,changes_new['page_title']==labels['id'],'left')\n",
    "instanceof = instanceof.withColumn(\"instanceof\", getValue(F.col(\"object\"))).drop('object')\n",
    "\n",
    "instanceof = instanceof.groupby('page_title').agg(F.collect_list('instanceof').alias('instanceof'),F.collect_list('rank').alias('rank'))\n",
    "changes_new = changes_new.join(instanceof,'page_title','left')\n",
    "\n",
    "# changes_new = changes_new.join(itemsWithSiteLink, changes_new['page_title']==itemsWithSiteLink['item_id'])  # Avoid filtering data from the holdout test\n",
    "\n",
    "\n",
    "######### More additional features\n",
    "def listToOneHot(df,tag_list_col,exlude_tags=[], include_tags=None):\n",
    "    tags = [\n",
    "        x[0]\n",
    "        for x in df.select(F.explode(tag_list_col).alias(tag_list_col))\n",
    "        .distinct()\n",
    "        .orderBy(tag_list_col)\n",
    "        .collect()\n",
    "    ]\n",
    "    if include_tags:\n",
    "        tags = include_tags\n",
    "    else:\n",
    "        tags = [tag for tag in tags if tag not in exlude_tags]\n",
    "    print(tags)\n",
    "    df = df.select(\n",
    "        \"*\",\n",
    "        *[\n",
    "            F.array_contains(tag_list_col, tag).alias(f\"{tag_list_col.replace(' ', '-')}-{tag.replace(' ', '-')}\").cast(\"integer\")\n",
    "            for tag in tags\n",
    "        ]\n",
    "    )\n",
    "    df = df.drop(tag_list_col)\n",
    "    return df\n",
    "\n",
    "changes_new.cache()\n",
    "\n",
    "changes_new = listToOneHot(changes_new,'event_user_groups')\n",
    "changes_new = listToOneHot(changes_new,'event_user_is_bot_by')\n",
    "include_tags = [\n",
    "       'revision_tags-InfoboxExport gadget',\n",
    "       'revision_tags-Unexpected value for IMDb identifier',\n",
    "       'revision_tags-Unexpected value for IUCN conservation status',\n",
    "       'revision_tags-Unexpected value for gender',\n",
    "       'revision_tags-deprecated property',\n",
    "       'revision_tags-description-phone-number', 'revision_tags-emoji',\n",
    "       'revision_tags-mw-replace',\n",
    "       'revision_tags-new editor changing statement',\n",
    "       'revision_tags-new editor removing statement',\n",
    "       'revision_tags-possible test edit', 'revision_tags-possible vandalism',\n",
    "       'revision_tags-removal of gender property',\n",
    "       'revision_tags-removing deprecated statement',\n",
    "       'revision_tags-self-referencing',\n",
    "       'revision_tags-unexpected value for phone number',\n",
    "       'revision_tags-unsourced ethnicity addition by IP',\n",
    "       'revision_tags-wikisyntax']\n",
    "changes_new = listToOneHot(changes_new,'revision_tags', exlude_tags=['mw-manual-revert','mw-reverted','wikidata-ui'], include_tags=include_tags)\n",
    "\n",
    "# changing columns to dump intermediate results\n",
    "for c in changes_new.columns:\n",
    "    changes_new = changes_new.withColumnRenamed(c, c.replace(\" \", \"-\"))\n",
    "\n",
    "changes_new.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13897a-80f3-4af2-8640-b4fb60cfffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dump data to parquet:\n",
    "dump_name = f\"wikidata_{snapshot_big}_training_sample_full\"\n",
    "changes_new = changes_new.repartition(256, \"revision_id\")\n",
    "changes_new.write.parquet(dump_name + \".parquet\", mode=\"overwrite\")\n",
    "# dump data to csv:\n",
    "changes_new = spark.read.parquet(dump_name + \".parquet\")\n",
    "changes_new_df = changes_new.toPandas()\n",
    "changes_new_df.to_csv(f\"../data/2024-04_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e579e23c-2542-407c-bee1-85051103b084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6689025"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43c6e95e-0148-4a41-9317-a234e54b7248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1296"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(changes_new_df[changes_new_df.revision_id.isin(revisions_to_check)].revision_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ebf53d-dacc-4f95-b830-721fa649bc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296\n"
     ]
    }
   ],
   "source": [
    "print(len(changes_new_df[changes_new_df.revision_id.isin(revisions_to_check)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ebcee-d20c-405f-8844-c23036ef71d2",
   "metadata": {},
   "source": [
    "## Getting ORES scores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcc147-9c2d-496e-943c-700110b56722",
   "metadata": {},
   "outputs": [],
   "source": [
    "ores = spark.sql('''SELECT rev_id as revision_id, scores.damaging.probability['true'] as pred FROM event_sanitized.mediawiki_revision_score\n",
    "                 WHERE database ='wikidatawiki' AND year IN (2021, 2022, 2023, 2024)  AND page_namespace=0''') #getting ORES data to compare with test \n",
    "orespd = ores.join(changes_new.select('revision_id'), 'revision_id').toPandas()\n",
    "orespd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0541d-6ea8-4014-97bd-9f379fcd05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "orespd.to_csv(\"../data/ores_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594354e8-c5a0-45f3-81a7-3c77c9c1bae1",
   "metadata": {},
   "source": [
    "## Getting text data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8bdb849-ea20-42f0-b5b4-bf0f29bdabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Optional, Union\n",
    "from deepdiff import DeepDiff\n",
    "\n",
    "\n",
    "def _parse_nested_change(old_dict: dict, new_dict: dict) -> Any:\n",
    "    hash_pattern = re.compile(r\".*\\['hash'\\]\")\n",
    "    id_pattern = re.compile(r\".*\\['id'\\]\")\n",
    "    diff = DeepDiff(\n",
    "        new_dict,\n",
    "        old_dict,\n",
    "        ignore_order=True,\n",
    "        exclude_regex_paths=[hash_pattern, id_pattern],\n",
    "    )\n",
    "    return diff.get(\"values_changed\", {})\n",
    "\n",
    "\n",
    "def _find_key_in_nested_dict(\n",
    "    dictionary: Union[dict, list], target_key: str\n",
    ") -> Optional[str]:\n",
    "    # If the input is a dictionary, search it\n",
    "    if isinstance(dictionary, dict):\n",
    "        # Iterate through each key-value pair in the dictionary\n",
    "        for key, value in dictionary.items():\n",
    "            # If the current value is another dictionary or list, recursively search it\n",
    "            if isinstance(value, (dict, list)):\n",
    "                nested_result = _find_key_in_nested_dict(value, target_key)\n",
    "                if nested_result is not None:\n",
    "                    return nested_result\n",
    "            # If the current key matches the target key, return the corresponding value\n",
    "            elif key == target_key:\n",
    "                return value\n",
    "    # If the input is a list, search each element in the list\n",
    "    elif isinstance(dictionary, list):\n",
    "        for item in dictionary:\n",
    "            if isinstance(item, (dict, list)):\n",
    "                nested_result = _find_key_in_nested_dict(item, target_key)\n",
    "                if nested_result is not None:\n",
    "                    return nested_result\n",
    "    else:\n",
    "        pass\n",
    "    # If the key is not found in the entire dictionary or list, return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_type_changes(type_changes_dict: dict) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Function to process diff in case type was changed from dict to list or vice versa\n",
    "    \"\"\"\n",
    "    adds = dict()\n",
    "    removes = dict()\n",
    "    list_to_look = [\"id\", \"value\", \"title\"]\n",
    "    try:\n",
    "        for key, value in type_changes_dict.items():\n",
    "            # case 1: old is empty list, new is dict\n",
    "            if len(value[\"old_value\"]) == 0 and len(value[\"new_value\"]) != 0:\n",
    "                # search for proporty\n",
    "                p = _find_key_in_nested_dict(value[\"new_value\"], \"property\")\n",
    "                # search for new value\n",
    "                for k in list_to_look:\n",
    "                    v = _find_key_in_nested_dict(value[\"new_value\"], k)\n",
    "                    if v:\n",
    "                        break\n",
    "                if p:\n",
    "                    adds[f\"{key}['{p}']\"] = v\n",
    "                else:\n",
    "                    adds[key] = v\n",
    "            # case 2: new is empty list, old is dict\n",
    "            if len(value[\"new_value\"]) == 0 and len(value[\"old_value\"]) != 0:\n",
    "                # search for proporty\n",
    "                p = _find_key_in_nested_dict(value[\"old_value\"], \"property\")\n",
    "                # search for deleted value\n",
    "                for k in list_to_look:\n",
    "                    v = _find_key_in_nested_dict(value[\"old_value\"], k)\n",
    "                    if v:\n",
    "                        break\n",
    "                if p:\n",
    "                    removes[f\"{key}['{p}']\"] = v\n",
    "                else:\n",
    "                    removes[key] = v\n",
    "    except Exception:\n",
    "        pass\n",
    "    return adds, removes\n",
    "\n",
    "\n",
    "def _add_iterable_items(result: Any) -> dict:\n",
    "    added_keys = result.keys()\n",
    "    parsed_result = {}\n",
    "    for k in added_keys:\n",
    "        parsed_result[k] = result[k]\n",
    "    return parsed_result\n",
    "\n",
    "\n",
    "def _get_element_by_path(obj: Any, path: str) -> str:\n",
    "    # Split the path and iterate through the keys to access the value\n",
    "    keys = re.findall(r\"\\['(.*?)'\\]\", path)\n",
    "    current_value = obj\n",
    "    for key in keys:\n",
    "        if key in current_value:\n",
    "            current_value = current_value[key]\n",
    "        else:\n",
    "            current_value = None\n",
    "            break\n",
    "    return current_value\n",
    "\n",
    "\n",
    "def parse_wikidata_revision_difference(\n",
    "    old_revision, new_revision\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Main function to parse the difference between two Wikidata revisions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse JSON strings into Python dictionaries if needed\n",
    "        if isinstance(old_revision, str):\n",
    "            dict_old = json.loads(old_revision)\n",
    "            dict_new = json.loads(new_revision)\n",
    "        else:\n",
    "            dict_old = old_revision\n",
    "            dict_new = new_revision\n",
    "\n",
    "        # Find the difference using deepdiff\n",
    "        hash_pattern = re.compile(r\"root\\['claims'\\]\\['.*?'\\]\\[\\d*\\].*\\['hash'\\]\")\n",
    "        id_pattern = re.compile(r\"root\\['claims'\\]\\['.*?'\\]\\[\\d*\\].*\\['id'\\]\")\n",
    "        diff = DeepDiff(\n",
    "            dict_old,\n",
    "            dict_new,\n",
    "            ignore_order=True,\n",
    "            exclude_regex_paths=[hash_pattern, id_pattern],\n",
    "        )\n",
    "\n",
    "        # Process the differences and create a structured representation\n",
    "        parsed_diff = {\n",
    "            \"added\": diff.get(\"dictionary_item_added\", []),\n",
    "            \"removed\": diff.get(\"dictionary_item_removed\", []),\n",
    "            \"type_changes\": diff.get(\"type_changes\", {}),\n",
    "            \"iterable_added\": _add_iterable_items(diff.get(\"iterable_item_added\", {})),\n",
    "            \"iterable_removed\": _add_iterable_items(\n",
    "                diff.get(\"iterable_item_removed\", {})\n",
    "            ),\n",
    "            \"changed\": diff.get(\"values_changed\", {}),\n",
    "        }\n",
    "\n",
    "        # Process type chenges:\n",
    "        additional_add, additional_removes = _parse_type_changes(\n",
    "            parsed_diff[\"type_changes\"]\n",
    "        )\n",
    "        \n",
    "        # check if descriptions are in changes (optional step): \n",
    "        is_desc = False\n",
    "        for r in parsed_diff[\"removed\"]:\n",
    "            if \"descriptions\" in r:\n",
    "                is_desc = True\n",
    "                break\n",
    "        for a in parsed_diff[\"added\"]:\n",
    "            if (\"descriptions\" in a) or is_desc:\n",
    "                is_desc = True\n",
    "                break\n",
    "        for c in parsed_diff[\"changed\"].keys():\n",
    "            if (\"descriptions\" in c) or is_desc:\n",
    "                is_desc = True\n",
    "                break\n",
    "        for c in parsed_diff[\"type_changes\"].keys():\n",
    "            if (\"descriptions\" in c) or is_desc:\n",
    "                is_desc = True\n",
    "                break\n",
    "        parsed_diff[\"descriptions\"] = dict_old[\"descriptions\"] if is_desc else {}\n",
    "        \n",
    "        # check if labels are in changes (optional step): \n",
    "        is_label = False\n",
    "        for r in parsed_diff[\"removed\"]:\n",
    "            if \"labels\" in r:\n",
    "                is_label = True\n",
    "                break\n",
    "        for a in parsed_diff[\"added\"]:\n",
    "            if (\"labels\" in a) or is_label:\n",
    "                is_label = True\n",
    "                break\n",
    "        for c in parsed_diff[\"changed\"].keys():\n",
    "            if (\"labels\" in c) or is_label:\n",
    "                is_label = True\n",
    "                break\n",
    "        for c in parsed_diff[\"type_changes\"].keys():\n",
    "            if (\"labels\" in c) or is_label:\n",
    "                is_label = True\n",
    "                break\n",
    "        parsed_diff[\"labels\"] = dict_old[\"labels\"] if is_label else {}\n",
    "        \n",
    "\n",
    "        # processing nested changes (only one level change):\n",
    "        processed_changes = {}\n",
    "        keys_to_delete = []\n",
    "        for key, value in parsed_diff[\"changed\"].items():\n",
    "            if isinstance(value[\"new_value\"], dict) and isinstance(\n",
    "                value[\"old_value\"], dict\n",
    "            ):\n",
    "                parsed_changes = _parse_nested_change(\n",
    "                    value[\"new_value\"], value[\"old_value\"]\n",
    "                )\n",
    "                keys_to_delete.append(key)\n",
    "                for key_c, value_c in parsed_changes.items():\n",
    "                    processed_changes[key + key_c] = value_c\n",
    "        parsed_diff[\"changed\"].update(processed_changes)\n",
    "        for k in keys_to_delete:\n",
    "            del parsed_diff[\"changed\"][k]\n",
    "\n",
    "        # getting removed values\n",
    "        precessed_removes = {}\n",
    "        for key in parsed_diff[\"removed\"]:\n",
    "            precessed_removes[key] = _get_element_by_path(dict_old, key)\n",
    "        parsed_diff[\"removed\"] = precessed_removes\n",
    "        parsed_diff[\"removed\"].update(parsed_diff[\"iterable_removed\"])\n",
    "        parsed_diff[\"removed\"].update(additional_removes)\n",
    "\n",
    "        # getting added values\n",
    "        precessed_add = {}\n",
    "        for key in parsed_diff[\"added\"]:\n",
    "            precessed_add[key] = _get_element_by_path(dict_new, key)\n",
    "        parsed_diff[\"added\"] = precessed_add\n",
    "        parsed_diff[\"added\"].update(parsed_diff[\"iterable_added\"])\n",
    "        parsed_diff[\"added\"].update(additional_add)\n",
    "\n",
    "        return [\n",
    "            str(parsed_diff[\"added\"]), str(parsed_diff[\"removed\"]), str(parsed_diff[\"changed\"]), \n",
    "            str(parsed_diff[\"descriptions\"]), str(parsed_diff[\"labels\"])]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return [str({}), str({}), str({}), str({}), str({})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a3e215-e819-4039-9205-257e0c43f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Filter columns + setup batching\n",
    "n_batch = 15\n",
    "selected_columns = [\n",
    "    'page_title', 'event_timestamp', 'event_user_is_anonymous', 'label_en',\n",
    "    \"revision_id\", \"revision_parent_id\", \n",
    "    \"revision_is_identity_reverted\", \"self_revert\", \n",
    "    \"reverting_revision_is_reverted_revision\"]\n",
    "\n",
    "changes_new = changes_new.select(selected_columns).withColumn(\"batch_id\", (rand() * n_batch + 1).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e6e2a9d-05dc-4316-8c47-84562dd09540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check the recent partitions\n",
    "# spark.sql('SHOW PARTITIONS wmf.mediawiki_wikitext_history').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96670d83-fe54-4d1b-91ad-b9d72c57e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developing functions to take a diffs\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"added\", T.StringType(), True),\n",
    "    T.StructField(\"removed\", T.StringType(), True),\n",
    "    T.StructField(\"changed\", T.StringType(), True),\n",
    "    T.StructField(\"descriptions\", T.StringType(), True),\n",
    "    T.StructField(\"labels\", T.StringType(), True),\n",
    "])\n",
    "udf_get_changes = F.udf(parse_wikidata_revision_difference, schema)\n",
    "\n",
    "\n",
    "for i in range(1, n_batch+1):\n",
    "    # Step 1: Get ids to filter:\n",
    "    changes_new_sample = changes_new.where(changes_new.batch_id==i)\n",
    "    print(\"Lenght of batch: \", changes_new_sample.count())\n",
    "    \n",
    "    selected_revision_df = changes_new_sample.select(F.col(\"revision_parent_id\").alias(\"revision_id\")) \\\n",
    "        .union(changes_new_sample.select(F.col(\"revision_id\").alias(\"revision_id\"))\n",
    "    ).distinct()\n",
    "    \n",
    "    print(f\"Processing batch {i}...\")\n",
    "    print(\"Number of revisions to look for: \", selected_revision_df.count())\n",
    "    \n",
    "    # Step 2: Get texts for selected revisions\n",
    "    wikitext_df = spark.sql(f\"\"\"SELECT revision_id, revision_text \n",
    "        FROM wmf.mediawiki_wikitext_history\n",
    "        WHERE snapshot = '{snapshot_big}' \n",
    "        AND wiki_db = \"wikidatawiki\" \n",
    "        AND page_namespace = 0 \n",
    "        \"\"\")\n",
    "    # Perform a broadcast join and filter the DataFrame\n",
    "    wikitext_df = (\n",
    "        wikitext_df\n",
    "        .join(broadcast(selected_revision_df), \"revision_id\", \"inner\")\n",
    "        .select(\"revision_id\", \"revision_text\")\n",
    "    ).dropDuplicates(subset=[\"revision_id\"])\n",
    "    # print(\"Number of revisions found: \", wikitext_df.count())\n",
    "    \n",
    "    # Step 3: Merging logic implementation: \n",
    "    history_columns = changes_new_sample.columns\n",
    "    revisions_text_1 = wikitext_df.alias(\"text\") \\\n",
    "        .join(\n",
    "        broadcast(changes_new_sample.alias(\"history\")),\n",
    "        F.col(\"text.revision_id\") == F.col(\"history.revision_id\"),\n",
    "        \"right\"\n",
    "    ).select(\n",
    "        *[F.col(f\"history.{c}\") for c in history_columns],\n",
    "        F.col(\"text.revision_text\").alias(\"revision_id_revision_text\"),\n",
    "    ).alias(\"t1\")\n",
    "\n",
    "    revisions_text_2 = wikitext_df.alias(\"text\") \\\n",
    "        .join(\n",
    "        broadcast(changes_new_sample.alias(\"history\")),\n",
    "        F.col(\"text.revision_id\") == F.col(\"history.revision_parent_id\"),\n",
    "        \"right\"\n",
    "    ).select(\n",
    "        F.col(f\"history.revision_id\"),\n",
    "        F.col(\"text.revision_text\").alias(\"revision_parent_id_revision_text\")\n",
    "    ).alias(\"t2\")\n",
    "\n",
    "    revisions_text_all = revisions_text_1 \\\n",
    "        .join(revisions_text_2, F.col(\"t1.revision_id\") == F.col(\"t2.revision_id\")) \\\n",
    "        .select(\n",
    "        F.col(\"t1.revision_id_revision_text\").alias(\"revision_id_revision_text\"),\n",
    "        F.col(\"t2.revision_parent_id_revision_text\").alias(\"revision_parent_id_revision_text\"),\n",
    "        *[F.col(f\"t1.{c}\") for c in history_columns],\n",
    "    )\n",
    "    \n",
    "    # print(\"Number of revisions after merging: \", revisions_text_all.count())\n",
    "\n",
    "    # drop empty text revisions\n",
    "    revisions_text_all = revisions_text_all.where(F.col(\"revision_parent_id_revision_text\").isNotNull())\n",
    "    revisions_text_all = revisions_text_all.where(F.col(\"revision_id_revision_text\").isNotNull())\n",
    "\n",
    "    # print(\"Number of revisions after merging & filtering: \", revisions_text_all.count())\n",
    "    \n",
    "    # comment to work with sample only\n",
    "    revisions_text_all_sample = revisions_text_all #.repartition(1024, \"revision_id\")\n",
    "    revisions_text_all_sample = revisions_text_all_sample \\\n",
    "        .withColumn(\"udf_res\", udf_get_changes(\n",
    "        revisions_text_all_sample['revision_parent_id_revision_text'],\n",
    "        revisions_text_all_sample['revision_id_revision_text'],\n",
    "        )) \\\n",
    "        .select(\n",
    "            *history_columns,\n",
    "            F.col(\"udf_res.added\"),\n",
    "            F.col(\"udf_res.removed\"),\n",
    "            F.col(\"udf_res.changed\"),\n",
    "            F.col(\"udf_res.descriptions\"),\n",
    "            F.col(\"udf_res.labels\")\n",
    "        )\n",
    "    \n",
    "    # Step 4: Saving: \n",
    "    # dump data to parquet:\n",
    "    dump_name = f\"full_wikidata_{snapshot_big}_training_sample_batch_{i}\"\n",
    "    revisions_text_all_sample.write.parquet(dump_name + \".parquet\", mode=\"overwrite\")\n",
    "\n",
    "    # dump data to csv:\n",
    "    revisions_text_all_sample = spark.read.parquet(dump_name + \".parquet\")\n",
    "    revisions_text_all_sample_df = revisions_text_all_sample.toPandas()\n",
    "    revisions_text_all_sample_df.to_csv(f\"../data/{dump_name}.csv\", index=False)\n",
    "    print(\"Number of revisions saved: \", revisions_text_all_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6de30a-5f99-46d7-866c-e26b63952240",
   "metadata": {},
   "source": [
    "# Labels collection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfdf047-c7d2-4139-884b-534ef5b9d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "dfs = [pd.read_csv(f\"../data/{snapshot_big}_content_batch_{i}.csv\") for i in tqdm(range(1,16))]\n",
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "print(df.revision_id.value_counts())\n",
    "\n",
    "def extract_all_P(text): \n",
    "    pattern = r'P\\d+'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "def extract_all_Q(text): \n",
    "    pattern = r'Q\\d+'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "all_Qs = set(df.page_title)\n",
    "all_Ps = set()\n",
    "\n",
    "for text in tqdm(df.added.values):\n",
    "    all_Qs |= set(extract_all_Q(text))\n",
    "    all_Ps |= set(extract_all_P(text))\n",
    "for text in tqdm(df.removed.values):\n",
    "    all_Qs |= set(extract_all_Q(text))\n",
    "    all_Ps |= set(extract_all_P(text))\n",
    "for text in tqdm(df.changed.values):\n",
    "    all_Qs |= set(extract_all_Q(text))\n",
    "    all_Ps |= set(extract_all_P(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e5f1d-abb2-45a2-9129-4e88b740097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting list of labels\n",
    "labels = spark.sql(f\"\"\"SELECT id, labels.{lang} as label_en\n",
    "      FROM wmf.wikidata_entity\n",
    "      WHERE snapshot = '{snapshot_small}'\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948f7a5-74db-427b-9648-5e8d7ed56f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "ids_to_extract = list(all_Ps | all_Qs)\n",
    "\n",
    "# Define schema for DataFrame containing IDs\n",
    "schema = StructType([StructField(\"id\", StringType(), True)])\n",
    "# Create DataFrame containing IDs to filter by\n",
    "df_ids = spark.createDataFrame([(id_,) for id_ in ids_to_extract], schema)\n",
    "\n",
    "# Perform join operation\n",
    "filtered_labels = labels.join(df_ids, \"id\", \"inner\")\n",
    "\n",
    "# Saving all labels text\n",
    "labels_df = filtered_labels.toPandas()\n",
    "labels_df.head()\n",
    "\n",
    "print(\"Rate of None: \", labels_df.label_en.isna().mean())\n",
    "\n",
    "labels_df.to_csv(f\"../data/full_labels_{snapshot_big}_text_en.csv\", index=False)\n",
    "print(\"Number of labels saved: \", labels_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

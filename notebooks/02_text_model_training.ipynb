{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1f2f6-10d4-450e-8f02-0d9233125e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset, load_metric, Dataset, ClassLabel, set_caching_enabled\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv(f\"../data/2024-04_content_batch_{i}.csv\") for i in tqdm(range(1,16))]\n",
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "print([len(d) for d in dfs])\n",
    "print(len(df))\n",
    "\n",
    "ids_to_drop = df[(df.revision_is_identity_reverted == True) & ((df.self_revert) | (df.reverting_revision_is_reverted_revision))][\"revision_id\"].values\n",
    "print(len(ids_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e9c9ed-2c6e-4e08-9da9-76e6b8bdae4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "revision_id\n",
       "1482024812    1\n",
       "1665607121    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.revision_id.value_counts().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e517f996-d2f8-449f-8218-e89d113f838f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_add</th>\n",
       "      <th>is_remove</th>\n",
       "      <th>is_change</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.764360</td>\n",
       "      <td>61297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.292103</td>\n",
       "      <td>1093665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.350654</td>\n",
       "      <td>530317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.205924</td>\n",
       "      <td>33692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.105580</td>\n",
       "      <td>4603084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.134176</td>\n",
       "      <td>60540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.170537</td>\n",
       "      <td>8784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.178896</td>\n",
       "      <td>19257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                mean    count\n",
       "is_add is_remove is_change                   \n",
       "False  False     False      0.764360    61297\n",
       "                 True       0.292103  1093665\n",
       "       True      False      0.350654   530317\n",
       "                 True       0.205924    33692\n",
       "True   False     False      0.105580  4603084\n",
       "                 True       0.134176    60540\n",
       "       True      False      0.170537     8784\n",
       "                 True       0.178896    19257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"is_add\"] = df.added.apply(lambda x: x != \"{}\")\n",
    "df[\"is_remove\"] = df.removed.apply(lambda x: x != \"{}\")\n",
    "df[\"is_change\"] = df.changed.apply(lambda x: x != \"{}\")\n",
    "df[\"is_labels\"] = df.labels.apply(lambda x: x != \"{}\")\n",
    "df[\"is_descriptions\"] = df.descriptions.apply(lambda x: x != \"{}\")\n",
    "\n",
    "df.groupby([\"is_add\", \"is_remove\", \"is_change\"]).revision_is_identity_reverted.agg([\"mean\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb170b2-2b62-4161-89e1-948a4e7ba366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_user_is_anonymous\n",
       "False    0.117665\n",
       "True     0.558898\n",
       "Name: revision_is_identity_reverted, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"event_user_is_anonymous\").revision_is_identity_reverted.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec4e642-1be9-45eb-8008-734619776fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_labels</th>\n",
       "      <th>is_descriptions</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.183386</td>\n",
       "      <td>4806467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.102222</td>\n",
       "      <td>713262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.110009</td>\n",
       "      <td>869145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.381629</td>\n",
       "      <td>21762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mean    count\n",
       "is_labels is_descriptions                   \n",
       "False     False            0.183386  4806467\n",
       "          True             0.102222   713262\n",
       "True      False            0.110009   869145\n",
       "          True             0.381629    21762"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"is_labels\", \"is_descriptions\"]).revision_is_identity_reverted.agg([\"mean\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060b17d-0387-4d12-8920-c185b46ea4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1259761/3232790573.py:2: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cat = pd.read_csv(\"data/2024-04_metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "holdout_test = pd.read_csv(\"../data/holdout/test_holdout.csv\", sep=\"\\t\")\n",
    "df_cat = pd.read_csv(\"../data/2024-04_metadata.csv\")\n",
    "# df_cat = df_cat[df_cat.revision_id.isin(set(holdout_test.rev))]\n",
    "\n",
    "revs_check = set(holdout_test.rev)\n",
    "dict_real = {k: v for k,v in zip(holdout_test.rev, holdout_test.label)}\n",
    "# tmp = df[df.revision_id.isin(revs_check)][[\"revision_id\", \"revision_is_identity_reverted\"]]\n",
    "# tmp[\"real\"] = tmp.revision_id.map(dict_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5727eb-96b9-4f6e-8214-7e2bf0aa260e",
   "metadata": {},
   "source": [
    "# Building train dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af40f8-305c-40ef-ab56-a0351054dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dict with IDs to description translation: \n",
    "labels = pd.read_csv(\"../data/full_labels_2024-04_text_en.csv\")\n",
    "labels_dict = {k:v for k, v in zip(labels['id'], labels.label_en) if not pd.isna(v)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ec2332-f0b9-442d-9248-fd4e89493dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_title</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>event_user_is_anonymous</th>\n",
       "      <th>label_en</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>revision_parent_id</th>\n",
       "      <th>revision_is_identity_reverted</th>\n",
       "      <th>self_revert</th>\n",
       "      <th>reverting_revision_is_reverted_revision</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>added</th>\n",
       "      <th>removed</th>\n",
       "      <th>changed</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>labels</th>\n",
       "      <th>is_add</th>\n",
       "      <th>is_remove</th>\n",
       "      <th>is_change</th>\n",
       "      <th>is_labels</th>\n",
       "      <th>is_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q108120268</td>\n",
       "      <td>2021-08-16 19:52:57</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Richard D. Walk\"</td>\n",
       "      <td>1482024812</td>\n",
       "      <td>1482024690</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"root['claims']['P140'][0]['references']\": None}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q23730171</td>\n",
       "      <td>2021-08-17 15:02:39</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Santa Lucía\"</td>\n",
       "      <td>1482702064</td>\n",
       "      <td>1482702008</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"root['descriptions']['ceb']\": {'language': '...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'sv': {'language': 'sv', 'value': 'ort i Colo...</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q33773</td>\n",
       "      <td>2021-08-17 18:26:32</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Convention of Moss\"</td>\n",
       "      <td>1482803316</td>\n",
       "      <td>1482802553</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"root['claims']['P710'][1]\": {'mainsnak': {'s...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q268797</td>\n",
       "      <td>2021-08-19 15:11:32</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Cloz\"</td>\n",
       "      <td>1483872645</td>\n",
       "      <td>1483872610</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"root['claims']['P31'][1]['qualifiers']\": Non...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q23657383</td>\n",
       "      <td>2021-08-20 09:28:57</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Timothy P. Trella\"</td>\n",
       "      <td>1484377279</td>\n",
       "      <td>1484377260</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"root['claims']['P734']\": [{'mainsnak': {'sna...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_title      event_timestamp event_user_is_anonymous  \\\n",
       "0  Q108120268  2021-08-16 19:52:57                   False   \n",
       "1   Q23730171  2021-08-17 15:02:39                   False   \n",
       "2      Q33773  2021-08-17 18:26:32                   False   \n",
       "3     Q268797  2021-08-19 15:11:32                   False   \n",
       "4   Q23657383  2021-08-20 09:28:57                   False   \n",
       "\n",
       "               label_en  revision_id  revision_parent_id  \\\n",
       "0     \"Richard D. Walk\"   1482024812          1482024690   \n",
       "1         \"Santa Lucía\"   1482702064          1482702008   \n",
       "2  \"Convention of Moss\"   1482803316          1482802553   \n",
       "3                \"Cloz\"   1483872645          1483872610   \n",
       "4   \"Timothy P. Trella\"   1484377279          1484377260   \n",
       "\n",
       "   revision_is_identity_reverted self_revert  \\\n",
       "0                          False         NaN   \n",
       "1                          False         NaN   \n",
       "2                          False         NaN   \n",
       "3                          False         NaN   \n",
       "4                          False         NaN   \n",
       "\n",
       "  reverting_revision_is_reverted_revision  batch_id  \\\n",
       "0                                     NaN         1   \n",
       "1                                     NaN         1   \n",
       "2                                     NaN         1   \n",
       "3                                     NaN         1   \n",
       "4                                     NaN         1   \n",
       "\n",
       "                                               added  \\\n",
       "0  {\"root['claims']['P140'][0]['references']\": None}   \n",
       "1  {\"root['descriptions']['ceb']\": {'language': '...   \n",
       "2  {\"root['claims']['P710'][1]\": {'mainsnak': {'s...   \n",
       "3                                                 {}   \n",
       "4  {\"root['claims']['P734']\": [{'mainsnak': {'sna...   \n",
       "\n",
       "                                             removed changed  \\\n",
       "0                                                 {}      {}   \n",
       "1                                                 {}      {}   \n",
       "2                                                 {}      {}   \n",
       "3  {\"root['claims']['P31'][1]['qualifiers']\": Non...      {}   \n",
       "4                                                 {}      {}   \n",
       "\n",
       "                                        descriptions labels  is_add  \\\n",
       "0                                                 {}     {}    True   \n",
       "1  {'sv': {'language': 'sv', 'value': 'ort i Colo...     {}    True   \n",
       "2                                                 {}     {}    True   \n",
       "3                                                 {}     {}   False   \n",
       "4                                                 {}     {}    True   \n",
       "\n",
       "   is_remove  is_change  is_labels  is_descriptions  \n",
       "0      False      False      False            False  \n",
       "1      False      False      False             True  \n",
       "2      False      False      False            False  \n",
       "3       True      False      False            False  \n",
       "4      False      False      False            False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2832efd7-efc1-4743-b482-b124770c6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = deepcopy(df)\n",
    "\n",
    "# filtering 80% to leave part for classifier\n",
    "np.random.seed(42)\n",
    "unique_ids_all = list(df.page_title.unique())\n",
    "ids_train_lm = list(np.random.choice(unique_ids_all, size=int(0.8*len(unique_ids_all)), replace=False))\n",
    "# ids_test = list(set(unique_ids_all) - set(ids_train))\n",
    "\n",
    "# Filtering by timestamp (event_timestamp > X go to test)\n",
    "ids_to_drop_date = tmp_df[\n",
    "    (pd.to_datetime(tmp_df['event_timestamp']) > pd.to_datetime('2023-06-01')) | \n",
    "    (pd.to_datetime(tmp_df['event_timestamp']) < pd.to_datetime('2021-09-01'))\n",
    "].revision_id\n",
    "\n",
    "tmp_df[\"is_lm_train\"] = df.page_title.isin(ids_train_lm)\n",
    "tmp_df = tmp_df[tmp_df.is_lm_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705cac3-eae7-46d0-b61c-703ec9fe6855",
   "metadata": {},
   "source": [
    "### Build sentences for removes action: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce263a76-f9f9-41bc-8fe8-5b7d49b86ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern\n",
    "PATTERN_P = r\"P\\d+\"\n",
    "PATTERN_Q = r\"Q\\d+\"\n",
    "DEFAULT_VALUE = \"unknown\"\n",
    "\n",
    "\n",
    "def check_id_pattern(string: str) -> bool:\n",
    "    if re.match(PATTERN_P, string) or re.match(PATTERN_Q, string):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_important_wording(string: str) -> bool:\n",
    "    if string in [\n",
    "        \"amount\",\n",
    "        \"unit\",\n",
    "        \"time\",\n",
    "        \"timezone\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"altitude\",\n",
    "        \"text\",\n",
    "    ]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_key(key: str):\n",
    "    pattern = r\"\\[\\'(.*?)\\'\\]\"\n",
    "    matches = re.findall(pattern, key)\n",
    "    if matches:\n",
    "        return matches\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def remove_wikilink(link: str) -> str:\n",
    "    link = str(link)\n",
    "    return link.replace(\"http://www.wikidata.org/entity/\", \"\")\n",
    "\n",
    "\n",
    "def get_value_by_type(json: dict, datatype: str):\n",
    "    if datatype == \"wikibase-entityid\":\n",
    "        try:\n",
    "            return [datatype, json[\"value\"][\"id\"]]\n",
    "        except:  # noqa: E722\n",
    "            # ToDo: fix this minor processing case\n",
    "            return [datatype, DEFAULT_VALUE]\n",
    "    elif datatype == \"string\":\n",
    "        return [datatype, json[\"value\"]]\n",
    "    elif datatype == \"globecoordinate\":\n",
    "        return [\n",
    "            datatype,\n",
    "            json[\"value\"][\"latitude\"],\n",
    "            json[\"value\"][\"longitude\"],\n",
    "            json[\"value\"][\"altitude\"],\n",
    "        ]\n",
    "    elif datatype == \"monolingualtext\":\n",
    "        return [datatype, json[\"value\"][\"text\"]]\n",
    "    elif datatype == \"time\":\n",
    "        return [datatype, json[\"value\"][\"time\"], json[\"value\"][\"timezone\"]]\n",
    "    elif datatype == \"quantity\":\n",
    "        return [\n",
    "            datatype,\n",
    "            json[\"value\"][\"amount\"],\n",
    "            remove_wikilink(json[\"value\"][\"unit\"]),\n",
    "        ]\n",
    "    else:\n",
    "        return [datatype]\n",
    "\n",
    "\n",
    "def process_sentence(items, labels_dict: dict) -> str:\n",
    "    items_transformed = [\n",
    "        labels_dict.get(i, DEFAULT_VALUE if check_id_pattern(i) else i) for i in items\n",
    "    ]\n",
    "    return \" \".join(items_transformed)\n",
    "\n",
    "\n",
    "def process_alteration(\n",
    "    left_q_id: str,\n",
    "    alterations: str,\n",
    "    action_type: str = \"remove: \",\n",
    "    labels_dict: dict = {},\n",
    "):\n",
    "    initial_sentence = [\n",
    "        action_type,\n",
    "        left_q_id if not pd.isna(left_q_id) else DEFAULT_VALUE,\n",
    "    ]\n",
    "    v_tmp = ast.literal_eval(alterations)\n",
    "    sentences = []\n",
    "    for key in v_tmp.keys():\n",
    "        sentence_key = deepcopy(initial_sentence)\n",
    "        sentence_key += process_key(key)\n",
    "        if \"sitelinks\" in sentence_key:  # skipping for now\n",
    "            continue\n",
    "        elif (\n",
    "            (\"aliases\" in sentence_key)\n",
    "            or (\"labels\" in sentence_key)\n",
    "            or (\"descriptions\" in sentence_key)\n",
    "        ):\n",
    "            if isinstance(v_tmp[key], list):\n",
    "                for el in v_tmp[key]:\n",
    "                    sentence_copy = deepcopy(sentence_key)\n",
    "                    sentence_copy.append(el[\"value\"])\n",
    "                    sentences.append(process_sentence(sentence_copy, labels_dict))\n",
    "            elif isinstance(v_tmp[key], str):\n",
    "                sentence_copy = deepcopy(sentence_key)\n",
    "                sentence_copy.append(v_tmp[key])\n",
    "                sentences.append(process_sentence(sentence_copy, labels_dict))\n",
    "            else:\n",
    "                sentence_copy = deepcopy(sentence_key)\n",
    "                sentence_copy.append(v_tmp[key][\"value\"])\n",
    "                sentences.append(process_sentence(sentence_copy, labels_dict))\n",
    "        elif (\"claims\" in sentence_key) and (\n",
    "            len(sentence_key) <= 4\n",
    "        ):  # skipping qualifiers case\n",
    "            if isinstance(v_tmp[key], list):\n",
    "                for el in v_tmp[key]:\n",
    "                    if el[\"mainsnak\"].get(\"datavalue\"):\n",
    "                        datatype = el[\"mainsnak\"][\"datavalue\"][\"type\"]\n",
    "                        if datatype in [\n",
    "                            \"string\",\n",
    "                            \"monolingualtext\",\n",
    "                            \"wikibase-entityid\",\n",
    "                        ]:\n",
    "                            sentence_copy = deepcopy(sentence_key)\n",
    "                            sentence_copy += get_value_by_type(\n",
    "                                el[\"mainsnak\"][\"datavalue\"], datatype\n",
    "                            )\n",
    "                            sentences.append(\n",
    "                                process_sentence(sentence_copy, labels_dict)\n",
    "                            )\n",
    "            elif isinstance(v_tmp[key], str):\n",
    "                sentence_copy = deepcopy(sentence_key)\n",
    "                sentence_copy.append(v_tmp[key])\n",
    "                sentences.append(process_sentence(sentence_copy, labels_dict))\n",
    "            else:\n",
    "                if v_tmp[key][\"mainsnak\"].get(\"datavalue\"):\n",
    "                    datatype = v_tmp[key][\"mainsnak\"][\"datavalue\"][\"type\"]\n",
    "                    if datatype in [\"string\", \"monolingualtext\", \"wikibase-entityid\"]:\n",
    "                        sentence_copy = deepcopy(sentence_key)\n",
    "                        sentence_copy += get_value_by_type(\n",
    "                            v_tmp[key][\"mainsnak\"][\"datavalue\"], datatype\n",
    "                        )\n",
    "                        sentences.append(process_sentence(sentence_copy, labels_dict))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def process_change(left_q_id: str, changes: str, action_type: str = \"change: \", labels_dict: dict = {}):\n",
    "    initial_sentence = [\n",
    "        action_type,\n",
    "        left_q_id if not pd.isna(left_q_id) else DEFAULT_VALUE,\n",
    "    ]\n",
    "    v_tmp = ast.literal_eval(changes)\n",
    "    sentences = []\n",
    "    for key in v_tmp.keys():\n",
    "        sentence_key = deepcopy(initial_sentence)\n",
    "        sentence_key += process_key(key)\n",
    "        if (\n",
    "            (\"aliases\" in sentence_key)\n",
    "            or (\"labels\" in sentence_key)\n",
    "            or (\"descriptions\" in sentence_key)\n",
    "        ):\n",
    "            sentence_copy_old, sentence_copy_new = deepcopy(sentence_key), deepcopy(\n",
    "                sentence_key\n",
    "            )\n",
    "            sentence_copy_old += [v_tmp[key][\"old_value\"]]\n",
    "            sentence_copy_new += [v_tmp[key][\"new_value\"]]\n",
    "            sentences.append(\n",
    "                (\n",
    "                    process_sentence(sentence_copy_old, labels_dict),\n",
    "                    process_sentence(sentence_copy_new[1:], labels_dict),\n",
    "                )\n",
    "            )\n",
    "        elif (\n",
    "            (\"claims\" in sentence_key)\n",
    "            and (\"qualifiers\" not in sentence_key)\n",
    "            and (\"rank\" not in sentence_key)\n",
    "        ):  # skipping qualifiers case\n",
    "            if \"string\" in sentence_key:\n",
    "                datatype = \"string\"\n",
    "            elif \"monolingualtext\" in sentence_key:\n",
    "                datatype = \"monolingualtext\"\n",
    "            elif \"claims\" in sentence_key:\n",
    "                datatype = \"claims\"\n",
    "            else:\n",
    "                datatype = \"skip\"\n",
    "            if datatype in [\"string\", \"monolingualtext\", \"claims\"]:\n",
    "                items_to_add = [\n",
    "                    i\n",
    "                    for i in sentence_key[4:]\n",
    "                    if check_id_pattern(i) or check_important_wording(i)\n",
    "                ]\n",
    "                sentence_copy_old, sentence_copy_new = deepcopy(\n",
    "                    sentence_key[:4]\n",
    "                ), deepcopy(sentence_key[:4])\n",
    "                if sentence_key[-1] == \"numeric-id\":\n",
    "                    new_value, old_value = (\n",
    "                        f\"Q{v_tmp[key]['new_value']}\",\n",
    "                        f\"Q{v_tmp[key]['old_value']}\",\n",
    "                    )\n",
    "                else:\n",
    "                    new_value, old_value = remove_wikilink(\n",
    "                        v_tmp[key][\"new_value\"]\n",
    "                    ), remove_wikilink(v_tmp[key][\"old_value\"])\n",
    "                sentence_copy_old += items_to_add + [old_value]\n",
    "                sentence_copy_new += items_to_add + [new_value]\n",
    "                sentence_copy_old, sentence_copy_new = [\n",
    "                    str(i) for i in sentence_copy_old\n",
    "                ], [str(i) for i in sentence_copy_new]\n",
    "                sentences.append(\n",
    "                    (\n",
    "                        process_sentence(sentence_copy_old, labels_dict),\n",
    "                        process_sentence(sentence_copy_new[1:], labels_dict),\n",
    "                    )\n",
    "                )\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98aedc6a-03e3-45db-a24f-76188ae47cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5131944it [01:21, 62817.52it/s]\n",
      "5131944it [04:44, 18013.36it/s]\n",
      "5131944it [01:40, 51229.51it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "revision_ids = []\n",
    "action_types = []    \n",
    "\n",
    "\n",
    "for q_id, rev_id, v, l in tqdm(zip(tmp_df.page_title, tmp_df.revision_id.values, tmp_df.removed.values, tmp_df.revision_is_identity_reverted.values)):\n",
    "    \n",
    "    action_type = \"remove: \"\n",
    "    sentences_found = process_alteration(left_q_id=q_id, alterations=v, action_type=action_type, labels_dict=labels_dict)\n",
    "    \n",
    "    sentences += sentences_found\n",
    "    labels += [l] * len(sentences_found)\n",
    "    revision_ids += [rev_id] * len(sentences_found)\n",
    "    action_types += [action_type] * len(sentences_found)\n",
    "    \n",
    "    \n",
    "for q_id, rev_id, v, l in tqdm(zip(tmp_df.page_title, tmp_df.revision_id.values, tmp_df.added.values, tmp_df.revision_is_identity_reverted.values)):\n",
    "    \n",
    "    action_type = \"add: \"\n",
    "    sentences_found = process_alteration(left_q_id=q_id, alterations=v, action_type=action_type, labels_dict=labels_dict)\n",
    "    \n",
    "    sentences += sentences_found\n",
    "    labels += [l] * len(sentences_found)\n",
    "    revision_ids += [rev_id] * len(sentences_found)\n",
    "    action_types += [action_type] * len(sentences_found)\n",
    "    \n",
    "    \n",
    "for q_id, rev_id, v, l in tqdm(zip(tmp_df.page_title, tmp_df.revision_id.values, tmp_df.changed.values, tmp_df.revision_is_identity_reverted.values)):\n",
    "    \n",
    "    action_type = \"change: \"\n",
    "    sentences_found = process_change(left_q_id=q_id, changes=v, action_type=action_type, labels_dict=labels_dict)\n",
    "    \n",
    "    sentences += sentences_found\n",
    "    labels += [l] * len(sentences_found)\n",
    "    revision_ids += [rev_id] * len(sentences_found)\n",
    "    action_types += [action_type] * len(sentences_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12826756-f4eb-407e-a9ad-7195b59f3249",
   "metadata": {},
   "source": [
    "### Build sentences for changed action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227cf3e-962b-4b27-baea-87820bd76535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495966\n"
     ]
    }
   ],
   "source": [
    "df_text = pd.DataFrame(\n",
    "    {\n",
    "        \"text_1\": [s[0] if isinstance(s, tuple) else s for s in sentences], \n",
    "        \"text_2\": [s[1] if isinstance(s, tuple) else \" \" for s in sentences],\n",
    "        \"label\": labels,\n",
    "        \"revision_id\": revision_ids,\n",
    "        \"action_type\": action_types\n",
    "    }\n",
    ")\n",
    "\n",
    "# filtering\n",
    "df_text = df_text[~df_text.revision_id.isin(revs_check)]  # filtering holdout\n",
    "df_text = df_text[~df_text.revision_id.isin(ids_to_drop_date)]  # filtering by date\n",
    "df_text = df_text[~df_text.revision_id.isin(ids_to_drop)]  # filtering by redundunt target\n",
    "\n",
    "# balancing\n",
    "data_ones = df_text[df_text.label == 1]\n",
    "data_zeros = df_text[df_text.label == 0].sample(len(data_ones), random_state=42)\n",
    "data = pd.concat([data_ones, data_zeros]).reset_index(drop=True)\n",
    "\n",
    "# Saving\n",
    "data.to_csv(\"../data/mlm_training_data_full.csv\", index=False)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859a24c-289c-49fa-9688-75f098e60d60",
   "metadata": {},
   "source": [
    "### Training the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee930f-41b7-4090-8aea-d848e5811981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 495966 examples [00:01, 327814.25 examples/s]\n",
      "Casting the dataset: 100%|██████████| 495966/495966 [00:00<00:00, 1135724.81 examples/s]\n",
      "/srv/home/trokhymovych/venv_new/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 471167/471167 [01:00<00:00, 7841.86 examples/s] \n",
      "Map: 100%|██████████| 24799/24799 [00:03<00:00, 7946.08 examples/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_dataset = Dataset.from_csv(\"../data/mlm_training_data_full.csv\")\n",
    "feat_class = ClassLabel(num_classes=2)\n",
    "training_dataset = training_dataset.cast_column(\"label\", feat_class)\n",
    "training_dataset = training_dataset.train_test_split(test_size=0.05, stratify_by_column=\"label\", shuffle=True, seed=42)\n",
    "\n",
    "# tokenization:\n",
    "model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "sentence1_key = \"text_1\"\n",
    "sentence2_key = \"text_2\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=512)\n",
    "\n",
    "encoded_dataset = training_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "num_labels = 2\n",
    "metric_name = \"accuracy\"\n",
    "batch_size = 8\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb38382-f137-4229-bb0a-07923604adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"../models/bert\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97245545-7bb7-494c-a0f1-f7847659fcf0",
   "metadata": {},
   "source": [
    "# Calculating text scores for other revisions (needed for later catboost training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bbb0a-b879-4dff-9c59-7f5c3d3dc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = deepcopy(df)\n",
    "\n",
    "# filtering 80% to leave part for classifier\n",
    "np.random.seed(42)\n",
    "unique_ids_all = list(df.page_title.unique())\n",
    "ids_train_lm = list(np.random.choice(unique_ids_all, size=int(0.8*len(unique_ids_all)), replace=False))\n",
    "# ids_test = list(set(unique_ids_all) - set(ids_train))\n",
    "\n",
    "tmp_df[\"is_lm_train\"] = df.page_title.isin(ids_train_lm)\n",
    "tmp_df = tmp_df[~tmp_df.is_lm_train | (pd.to_datetime(tmp_df['event_timestamp']) > pd.to_datetime('2023-06-01'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68808bb7-b5e2-44ff-ad4f-9606b7c45557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2598093it [00:57, 45244.68it/s]\n",
      "2598093it [03:14, 13342.99it/s]\n",
      "2598093it [01:06, 38944.21it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "revision_ids = []\n",
    "action_types = []    \n",
    "\n",
    "\n",
    "for q_id, rev_id, v, l in tqdm(zip(tmp_df.page_title, tmp_df.revision_id.values, tmp_df.removed.values, tmp_df.revision_is_identity_reverted.values)):\n",
    "    \n",
    "    action_type = \"remove: \"\n",
    "    sentences_found = process_alteration(left_q_id=q_id, alterations=v, action_type=action_type, labels_dict=labels_dict)\n",
    "    \n",
    "    sentences += sentences_found\n",
    "    labels += [l] * len(sentences_found)\n",
    "    revision_ids += [rev_id] * len(sentences_found)\n",
    "    action_types += [action_type] * len(sentences_found)\n",
    "    \n",
    "    \n",
    "for q_id, rev_id, v, l in tqdm(zip(tmp_df.page_title, tmp_df.revision_id.values, tmp_df.added.values, tmp_df.revision_is_identity_reverted.values)):\n",
    "    \n",
    "    action_type = \"add: \"\n",
    "    sentences_found = process_alteration(left_q_id=q_id, alterations=v, action_type=action_type, labels_dict=labels_dict)\n",
    "    \n",
    "    sentences += sentences_found\n",
    "    labels += [l] * len(sentences_found)\n",
    "    revision_ids += [rev_id] * len(sentences_found)\n",
    "    action_types += [action_type] * len(sentences_found)\n",
    "    \n",
    "    \n",
    "for q_id, rev_id, v, l in tqdm(zip(tmp_df.page_title, tmp_df.revision_id.values, tmp_df.changed.values, tmp_df.revision_is_identity_reverted.values)):\n",
    "    \n",
    "    action_type = \"change: \"\n",
    "    sentences_found = process_change(left_q_id=q_id, changes=v, action_type=action_type, labels_dict=labels_dict)\n",
    "    \n",
    "    sentences += sentences_found\n",
    "    labels += [l] * len(sentences_found)\n",
    "    revision_ids += [rev_id] * len(sentences_found)\n",
    "    action_types += [action_type] * len(sentences_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1516a3d-d400-418b-9e29-e4e2b42effeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2229877\n"
     ]
    }
   ],
   "source": [
    "df_text = pd.DataFrame(\n",
    "    {\n",
    "        \"text_1\": [s[0] if isinstance(s, tuple) else s for s in sentences], \n",
    "        \"text_2\": [s[1] if isinstance(s, tuple) else \" \" for s in sentences],\n",
    "        \"label\": labels,\n",
    "        \"revision_id\": revision_ids,\n",
    "        \"action_type\": action_types\n",
    "    }\n",
    ")\n",
    "\n",
    "print(len(df_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165b059-9f39-42cb-961b-b7278de77cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def preds_processing(preds):\n",
    "    res = []\n",
    "    for i in preds:\n",
    "        res.append(i[1]['score'] > 0.5)\n",
    "    return res\n",
    "\n",
    "def preds_processing_prob(preds):\n",
    "    res = []\n",
    "    for i in preds:\n",
    "        res.append(i[1]['score'])\n",
    "    return res\n",
    "\n",
    "texts_to_process = []\n",
    "\n",
    "for text_1, text_2 in zip(df_text.text_1.values, df_text.text_2.values):\n",
    "    if (text_2 == \" \"):\n",
    "        texts_to_process.append(text_1)\n",
    "    else:\n",
    "        texts_to_process.append({\"text\": text_1, \"text_pair\": text_2})\n",
    "      \n",
    "checkpoint = \"../models/bert/checkpoint-294480\"\n",
    "\n",
    "device = 0\n",
    "batch_size = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, truncation=True, max_length=512, device=device)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint).to(device)\n",
    "clf = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer, device=device, batch_size=batch_size)\n",
    "\n",
    "scores = []\n",
    "print(\"Number of text to process: \", len(texts_to_process))\n",
    "for i in tqdm(range(0, len(texts_to_process), 500)):\n",
    "    tokenizer_kwargs = {'truncation': True, 'max_length': 512}\n",
    "    preds = clf(texts_to_process[i:i+500], return_all_scores=True, **tokenizer_kwargs, batch_size=batch_size)\n",
    "    scores += preds_processing_prob(preds)\n",
    "\n",
    "print(\"Number of text scores: \", len(scores))\n",
    "\n",
    "df_text[\"scores\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5cdfac-8c03-474c-a686-b0921f501eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.to_csv(\"../data/mlm_text_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
